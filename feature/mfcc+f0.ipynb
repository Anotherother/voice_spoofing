{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pysptk\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import features as psf\n",
    "import scipy.io.wavfile as wav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nc = 20\n",
    "\n",
    "def generateMFCC(filename):\n",
    "    (rate,sig) = wav.read(filename)\n",
    "    mfcc_feat = psf.mfcc(sig, rate, numcep = nc)\n",
    "    numOfRow = mfcc_feat.shape[0]\n",
    "    \n",
    "    sum = np.empty([0, mfcc_feat.shape[1]])\n",
    "    sum = np.sum(mfcc_feat, axis = 0)\n",
    "    sum /= numOfRow\n",
    "    return sum\n",
    "\n",
    "def model_dictor( dataframe, directory_in, directory_out):\n",
    "    mfcc = []\n",
    "\n",
    "    for i in dataframe.index:\n",
    "\n",
    "        fileName = directory_in + dataframe.ix[i][0] + '\\\\' + dataframe.ix[i][1] + '.wav'\n",
    "\n",
    "        mfcc_res = generateMFCC(fileName)\n",
    "        mfcc.append(mfcc_res)\n",
    "    \n",
    "    return np.array(mfcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "directory = r'N://Science//Antispoofing Datasets//ASVSpoof2015//wav//'\n",
    "output_directory = r'N://Science//Antispoofing Research (ipython notebook)//gmm t-sne//mfcc_all//'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train data\n",
    "\n",
    "with open (\"./protocol/cm_train.trn\", \"r\") as myfile:\n",
    "    data=myfile.read().split('\\n')\n",
    "    \n",
    "for x in  range(len(data)): \n",
    "    data[x] =  data[x].split(' ')\n",
    "    \n",
    "dataframe = pd.DataFrame(data, columns=[\"dictor\", \"name\", \"algorithm\", \"sp_hu\"])\n",
    "uniquelabels = dataframe.algorithm.unique()\n",
    "\n",
    "for i in range(uniquelabels.shape[0]):\n",
    "    mfcc_output = model_dictor(dataframe[dataframe.algorithm == uniquelabels[i]], directory, output_directory)\n",
    "    np.savetxt('./mfcc_all/mfcc_train_'+ str(uniquelabels[i]) + '.txt', mfcc_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dev data\n",
    "\n",
    "with open (\"./protocol/cm_develop.ndx\", \"r\") as myfile:\n",
    "    data=myfile.read().split('\\n')\n",
    "    \n",
    "for x in  range(len(data)): \n",
    "    data[x] =  data[x].split(' ')\n",
    "    \n",
    "dataframe = pd.DataFrame(data, columns=[\"dictor\", \"name\", \"algorithm\", \"sp_hu\"])\n",
    "uniquelabels = dataframe.algorithm.unique()\n",
    "\n",
    "for i in range(uniquelabels.shape[0]):\n",
    "    mfcc_output = model_dictor(dataframe[dataframe.algorithm == uniquelabels[i]], directory, output_directory)\n",
    "    np.savetxt('./mfcc_all/mfcc_dev_'+ str(uniquelabels[i]) + '.txt', mfcc_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# eva data\n",
    "\n",
    "with open (\"./protocol/cm_evaluation.ndx\", \"r\") as myfile:\n",
    "    data=myfile.read().split('\\n')\n",
    "    \n",
    "for x in  range(len(data)): \n",
    "    data[x] =  data[x].split(' ')\n",
    "    \n",
    "dataframe = pd.DataFrame(data, columns=[\"dictor\", \"name\", \"algorithm\", \"sp_hu\"])\n",
    "uniquelabels = dataframe.algorithm.unique()\n",
    "\n",
    "for i in range(uniquelabels.shape[0]):\n",
    "    mfcc_output = model_dictor(dataframe[dataframe.algorithm == uniquelabels[i]], directory, output_directory)\n",
    "    np.savetxt('./mfcc_all/mfcc_eva_'+ str(uniquelabels[i]) + '.txt', mfcc_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mfcc_train_human = np.loadtxt('./mfcc_all/mfcc_train_human.txt')\n",
    "mfcc_train_S1 = np.loadtxt('./mfcc_all/mfcc_train_S1.txt')\n",
    "mfcc_train_S2 = np.loadtxt('./mfcc_all/mfcc_train_S2.txt')\n",
    "mfcc_train_S3 = np.loadtxt('./mfcc_all/mfcc_train_S3.txt')\n",
    "mfcc_train_S4 = np.loadtxt('./mfcc_all/mfcc_train_S4.txt')\n",
    "mfcc_train_S5 = np.loadtxt('./mfcc_all/mfcc_train_S5.txt')\n",
    "\n",
    "\n",
    "mfcc_eva_human = np.loadtxt('./mfcc_all/mfcc_eva_human.txt')\n",
    "mfcc_eva_S1 = np.loadtxt('./mfcc_all/mfcc_eva_S1.txt')\n",
    "mfcc_eva_S2 = np.loadtxt('./mfcc_all/mfcc_eva_S2.txt')\n",
    "mfcc_eva_S3 = np.loadtxt('./mfcc_all/mfcc_eva_S3.txt')\n",
    "mfcc_eva_S4 = np.loadtxt('./mfcc_all/mfcc_eva_S4.txt')\n",
    "mfcc_eva_S5 = np.loadtxt('./mfcc_all/mfcc_eva_S5.txt')\n",
    "mfcc_eva_S6 = np.loadtxt('./mfcc_all/mfcc_eva_S6.txt')\n",
    "mfcc_eva_S7 = np.loadtxt('./mfcc_all/mfcc_eva_S7.txt')\n",
    "mfcc_eva_S8 = np.loadtxt('./mfcc_all/mfcc_eva_S8.txt')\n",
    "mfcc_eva_S9 = np.loadtxt('./mfcc_all/mfcc_eva_S9.txt')\n",
    "mfcc_eva_S10 = np.loadtxt('./mfcc_all/mfcc_eva_S10.txt')\n",
    "\n",
    "\n",
    "mfcc_dev_human = np.loadtxt('./mfcc_all/mfcc_train_human.txt')\n",
    "mfcc_dev_S1 = np.loadtxt('./mfcc_all/mfcc_dev_S1.txt')\n",
    "mfcc_dev_S2 = np.loadtxt('./mfcc_all/mfcc_dev_S2.txt')\n",
    "mfcc_dev_S3 = np.loadtxt('./mfcc_all/mfcc_dev_S3.txt')\n",
    "mfcc_dev_S4 = np.loadtxt('./mfcc_all/mfcc_dev_S4.txt')\n",
    "mfcc_dev_S5 = np.loadtxt('./mfcc_all/mfcc_dev_S5.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mfcc_spoof_train = np.vstack([mfcc_train_S1,mfcc_train_S2,mfcc_train_S3,mfcc_train_S4,mfcc_train_S5])\n",
    "mfcc_spoof_dev = np.vstack([mfcc_dev_S1,mfcc_dev_S2,mfcc_dev_S3,mfcc_dev_S4,mfcc_dev_S5])\n",
    "mfcc_spoof_eva = np.vstack([mfcc_eva_S1,mfcc_eva_S2,mfcc_eva_S3,mfcc_eva_S4,mfcc_eva_S5 /\n",
    "                            mfcc_eva_S6,mfcc_eva_S7,mfcc_eva_S8,mfcc_eva_S9,mfcc_eva_S10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savetxt('./mfcc_all/mfcc_spoof_train.txt', mfcc_spoof_train)\n",
    "np.savetxt('./mfcc_all/mfcc_spoof_dev.txt', mfcc_spoof_dev)\n",
    "np.savetxt('./mfcc_all/mfcc_spoof_eva.txt', mfcc_spoof_eva)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hop_length = 80\n",
    "\n",
    "def extractF0( dataframe, directory_in, directory_out):\n",
    "    f0_all = []\n",
    "    \n",
    "    for i in dataframe.index:\n",
    "\n",
    "        fileName = directory_in + dataframe.ix[i][0] + '\\\\' + dataframe.ix[i][1] + '.wav'\n",
    "        \n",
    "        rate, audio = wav.read(fileName)\n",
    "        f0 = pysptk.swipe(audio.astype(np.float64), fs=rate, hopsize=hop_length, min=60, max=240, otype=\"f0\")\n",
    "        f0_all.append(f0)\n",
    "    \n",
    "    return np.array(f0_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train data\n",
    "\n",
    "with open (\"./protocol/cm_train.trn\", \"r\") as myfile:\n",
    "    data=myfile.read().split('\\n')\n",
    "    \n",
    "for x in  range(len(data)): \n",
    "    data[x] =  data[x].split(' ')\n",
    "    \n",
    "dataframe = pd.DataFrame(data, columns=[\"dictor\", \"name\", \"algorithm\", \"sp_hu\"])\n",
    "uniquelabels = dataframe.algorithm.unique()\n",
    "\n",
    "for i in range(uniquelabels.shape[0]):\n",
    "    f0_output = extractF0(dataframe[dataframe.algorithm == uniquelabels[i]], directory, output_directory)\n",
    "    np.savetxt('./f0/f0_train_'+ str(uniquelabels[i]) + '.txt', f0_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dev data\n",
    "\n",
    "with open (\"./protocol/cm_develop.ndx\", \"r\") as myfile:\n",
    "    data=myfile.read().split('\\n')\n",
    "    \n",
    "for x in  range(len(data)): \n",
    "    data[x] =  data[x].split(' ')\n",
    "    \n",
    "dataframe = pd.DataFrame(data, columns=[\"dictor\", \"name\", \"algorithm\", \"sp_hu\"])\n",
    "uniquelabels = dataframe.algorithm.unique()\n",
    "\n",
    "for i in range(uniquelabels.shape[0]):\n",
    "    f0_output = extractF0(dataframe[dataframe.algorithm == uniquelabels[i]], directory, output_directory)\n",
    "    np.savetxt('./f0/f0_dev_'+ str(uniquelabels[i]) + '.txt', f0_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# eva data\n",
    "\n",
    "with open (\"./protocol/cm_evaluation.ndx\", \"r\") as myfile:\n",
    "    data=myfile.read().split('\\n')\n",
    "    \n",
    "for x in  range(len(data)): \n",
    "    data[x] =  data[x].split(' ')\n",
    "    \n",
    "dataframe = pd.DataFrame(data, columns=[\"dictor\", \"name\", \"algorithm\", \"sp_hu\"])\n",
    "uniquelabels = dataframe.algorithm.unique()\n",
    "\n",
    "for i in range(uniquelabels.shape[0]):\n",
    "    f0_output = extractF0(dataframe[dataframe.algorithm == uniquelabels[i]], directory, output_directory)\n",
    "    np.savetxt('./f0/f0_eva_'+ str(uniquelabels[i]) + '.txt', f0_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open (\"./protocol/cm_evaluation.ndx\", \"r\") as myfile:\n",
    "    data=myfile.read().split('\\n')\n",
    "    \n",
    "for x in  range(len(data)): \n",
    "    data[x] =  data[x].split(' ')\n",
    "    \n",
    "dataframe = pd.DataFrame(data, columns=[\"dictor\", \"name\", \"algorithm\", \"sp_hu\"])\n",
    "uniquelabels = dataframe.algorithm.unique()\n",
    "\n",
    "for i in range(3,uniquelabels.shape[0]):\n",
    "    mfcc_output = model_dictor(dataframe[dataframe.algorithm == uniquelabels[i]], directory, output_directory)\n",
    "    np.savetxt('./mfcc_all/mfcc_eva_'+ str(uniquelabels[i]) + '.txt', mfcc_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
